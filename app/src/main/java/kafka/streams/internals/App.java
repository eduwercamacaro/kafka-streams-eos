/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package kafka.streams.internals;


import io.javalin.Javalin;
import net.datafaker.Faker;
import net.datafaker.service.RandomService;
import org.apache.kafka.clients.admin.AdminClient;
import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.common.utils.Bytes;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.kstream.Materialized;
import org.apache.kafka.streams.state.KeyValueStore;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.List;
import java.util.Properties;
import java.util.concurrent.CountDownLatch;

public class App {
    public static final String INPUT_TOPIC = "input-topic";
    public static final String OUTPUT_TOPIC = "output-topic";
    private static final Faker FAKER = new Faker();
    private static final RandomService RANDOM = FAKER.random();
    private static final Logger log = LoggerFactory.getLogger(App.class);


    public static void main(String[] args) throws Exception {
        final Properties props = AppConfig.streamsConfig(args);
        AdminClient client = AdminClient.create(props);
        NewTopic inputTopic = new NewTopic(INPUT_TOPIC, 15, (short) 1);
        NewTopic outputTopic = new NewTopic(OUTPUT_TOPIC, 15, (short) 1);
        client.createTopics(List.of(inputTopic, outputTopic));
        final StreamsBuilder builder = new StreamsBuilder();
        final KStream<String, Bytes> source = builder.stream(INPUT_TOPIC);
        Materialized<String, Bytes, KeyValueStore<Bytes, byte[]>> materialized = Materialized.
                <String, Bytes, KeyValueStore<Bytes, byte[]>>as("state")
                .withKeySerde(Serdes.String())
                .withValueSerde(Serdes.Bytes());
        source.groupByKey().aggregate(() -> new Bytes(new byte[0]), (key, value, aggregate) -> randomBytes(), materialized);
        final KafkaStreams streams = new KafkaStreams(builder.build(), props);
        HttpHandler httpHandler = new HttpHandler(streams);
        Javalin httpServer = Javalin.create().start(AppConfig.httpPort(args));
        httpServer.get("/state", httpHandler::state);

        CountDownLatch latch = new CountDownLatch(2);
        Runtime.getRuntime().addShutdownHook(new Thread("streams-shutdown-hook") {
            @Override
            public void run() {
                streams.close();
                latch.countDown();
            }
        });
        Runtime.getRuntime().addShutdownHook(new Thread("http-server-shutdown-hook") {
            @Override
            public void run() {
                httpServer.close();
                latch.countDown();
            }
        });

        try {
            streams.start();
            latch.await();
            log.info("Done");
        } catch (final Throwable e) {
            System.exit(1);
        }
    }

    private static Bytes randomBytes() {
        return new Bytes(RANDOM.nextRandomBytes(RANDOM.nextInt(10000, 20000)));
    }
}
